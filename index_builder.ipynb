{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Index Builder\n",
    "This notebook processes 60 Parquet files from GCS and builds the inverted index for search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.11' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q pyarrow pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate for GCS access\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload inverted_index_gcp.py to Colab\n",
    "# Make sure to update PROJECT_ID in inverted_index_gcp.py before uploading\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "print(\"Please upload inverted_index_gcp.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from google.cloud import storage\n",
    "from nltk.corpus import stopwords\n",
    "from inverted_index_gcp import InvertedIndex, MultiFileWriter\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from contextlib import closing\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "BUCKET_NAME = '208894444'\n",
    "NUM_PARQUET_FILES = 60\n",
    "OUTPUT_DIR = './index_output'\n",
    "\n",
    "# Create output directory\n",
    "!mkdir -p {OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization setup (must match search_frontend.py)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\/\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
    "                   \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
    "                   \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
    "                   \"many\", \"however\", \"would\", \"became\"]\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenize text and remove stopwords.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = [token.group().lower() for token in RE_WORD.finditer(text)]\n",
    "    return [token for token in tokens if token not in all_stopwords]\n",
    "\n",
    "# Test tokenization\n",
    "print(\"Test tokenization:\", tokenize(\"The quick brown fox jumps over the lazy dog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GCS client\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(BUCKET_NAME)\n",
    "\n",
    "# List all parquet files in the bucket\n",
    "blobs = list(bucket.list_blobs())\n",
    "parquet_files = [blob.name for blob in blobs if blob.name.endswith('.parquet')]\n",
    "print(f\"Found {len(parquet_files)} parquet files in bucket {BUCKET_NAME}\")\n",
    "print(\"First few files:\", parquet_files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main indexing loop\n",
    "print(\"Starting indexing process...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize data structures\n",
    "index_body = InvertedIndex()\n",
    "doc_lengths = {}  # wiki_id -> document length\n",
    "titles = {}       # wiki_id -> title\n",
    "pagerank = {}     # wiki_id -> pagerank score (placeholder)\n",
    "\n",
    "total_docs = 0\n",
    "total_tokens = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# Process each Parquet file\n",
    "for i, parquet_file in enumerate(parquet_files[:NUM_PARQUET_FILES], 1):\n",
    "    print(f\"\\n[{i}/{NUM_PARQUET_FILES}] Processing: {parquet_file}\")\n",
    "    \n",
    "    try:\n",
    "        # Download and read parquet file\n",
    "        blob = bucket.blob(parquet_file)\n",
    "        parquet_path = f'/tmp/{parquet_file.replace(\"/\", \"_\")}'\n",
    "        blob.download_to_filename(parquet_path)\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "        \n",
    "        print(f\"  Loaded {len(df)} documents\")\n",
    "        \n",
    "        # Process each document\n",
    "        for idx, row in df.iterrows():\n",
    "            # Extract fields\n",
    "            wiki_id = int(row.get('id', row.get('wiki_id', idx)))\n",
    "            title = str(row.get('title', ''))\n",
    "            text = str(row.get('text', ''))\n",
    "            \n",
    "            # Tokenize text\n",
    "            tokens = tokenize(text)\n",
    "            \n",
    "            if len(tokens) > 0:\n",
    "                # Add to inverted index\n",
    "                index_body.add_doc(wiki_id, tokens)\n",
    "                \n",
    "                # Store document length\n",
    "                doc_lengths[wiki_id] = len(tokens)\n",
    "                \n",
    "                # Store title\n",
    "                titles[wiki_id] = title\n",
    "                \n",
    "                # Placeholder PageRank (uniform distribution)\n",
    "                pagerank[wiki_id] = 1.0\n",
    "                \n",
    "                total_docs += 1\n",
    "                total_tokens += len(tokens)\n",
    "        \n",
    "        print(f\"  Total docs so far: {total_docs:,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR processing {parquet_file}: {e}\")\n",
    "        continue\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"Indexing complete!\")\n",
    "print(f\"Total documents: {total_docs:,}\")\n",
    "print(f\"Total tokens: {total_tokens:,}\")\n",
    "print(f\"Unique terms: {len(index_body.df):,}\")\n",
    "print(f\"Time elapsed: {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BM25 statistics\n",
    "N = total_docs\n",
    "avg_dl = total_tokens / total_docs if total_docs > 0 else 0\n",
    "\n",
    "bm25_data = {\n",
    "    'doc_lengths': doc_lengths,\n",
    "    'avg_dl': avg_dl,\n",
    "    'N': N\n",
    "}\n",
    "\n",
    "print(f\"BM25 Statistics:\")\n",
    "print(f\"  N (total docs): {N:,}\")\n",
    "print(f\"  Average doc length: {avg_dl:.2f} tokens\")\n",
    "print(f\"  Min doc length: {min(doc_lengths.values())}\")\n",
    "print(f\"  Max doc length: {max(doc_lengths.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write posting lists to binary files\n",
    "print(\"\\nWriting posting lists to binary files...\")\n",
    "\n",
    "# Group posting lists into buckets for parallel writing\n",
    "from itertools import islice\n",
    "\n",
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"\"\"Collect data into fixed-length chunks.\"\"\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip(*args)\n",
    "\n",
    "# Get all posting lists\n",
    "posting_lists = list(index_body._posting_list.items())\n",
    "print(f\"Total posting lists to write: {len(posting_lists):,}\")\n",
    "\n",
    "# Write posting lists using the static method\n",
    "bucket_data = (0, posting_lists)\n",
    "InvertedIndex.write_a_posting_list(bucket_data, OUTPUT_DIR, BUCKET_NAME)\n",
    "\n",
    "# Load the posting_locs that were just written\n",
    "posting_locs_path = f'{OUTPUT_DIR}/0_posting_locs.pickle'\n",
    "with open(posting_locs_path, 'rb') as f:\n",
    "    index_body.posting_locs = pickle.load(f)\n",
    "\n",
    "print(f\"Posting lists written successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save index_body.pkl\n",
    "print(\"\\nSaving index_body.pkl...\")\n",
    "index_body.write_index(OUTPUT_DIR, 'index_body', BUCKET_NAME)\n",
    "print(\"✓ index_body.pkl saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save bm25_data.pkl\n",
    "print(\"\\nSaving bm25_data.pkl...\")\n",
    "bm25_path = f'{OUTPUT_DIR}/bm25_data.pkl'\n",
    "with open(bm25_path, 'wb') as f:\n",
    "    pickle.dump(bm25_data, f)\n",
    "\n",
    "# Upload to GCS\n",
    "blob = bucket.blob('bm25_data.pkl')\n",
    "blob.upload_from_filename(bm25_path)\n",
    "print(\"✓ bm25_data.pkl saved and uploaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save titles.pkl\n",
    "print(\"\\nSaving titles.pkl...\")\n",
    "titles_path = f'{OUTPUT_DIR}/titles.pkl'\n",
    "with open(titles_path, 'wb') as f:\n",
    "    pickle.dump(titles, f)\n",
    "\n",
    "# Upload to GCS\n",
    "blob = bucket.blob('titles.pkl')\n",
    "blob.upload_from_filename(titles_path)\n",
    "print(f\"✓ titles.pkl saved and uploaded ({len(titles):,} entries)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pagerank.pkl (placeholder)\n",
    "print(\"\\nSaving pagerank.pkl (placeholder)...\")\n",
    "pagerank_path = f'{OUTPUT_DIR}/pagerank.pkl'\n",
    "with open(pagerank_path, 'wb') as f:\n",
    "    pickle.dump(pagerank, f)\n",
    "\n",
    "# Upload to GCS\n",
    "blob = bucket.blob('pagerank.pkl')\n",
    "blob.upload_from_filename(pagerank_path)\n",
    "print(f\"✓ pagerank.pkl saved and uploaded ({len(pagerank):,} entries)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all saved files\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"INDEX BUILDING COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nFiles saved to GCS bucket:\", BUCKET_NAME)\n",
    "print(\"  ✓ index_body.pkl\")\n",
    "print(\"  ✓ index_body_000.bin (and other .bin files)\")\n",
    "print(\"  ✓ bm25_data.pkl\")\n",
    "print(\"  ✓ titles.pkl\")\n",
    "print(\"  ✓ pagerank.pkl\")\n",
    "print(\"\\nStatistics:\")\n",
    "print(f\"  Total documents: {N:,}\")\n",
    "print(f\"  Unique terms: {len(index_body.df):,}\")\n",
    "print(f\"  Average doc length: {avg_dl:.2f} tokens\")\n",
    "print(\"\\nYou can now use these files in search_frontend.py!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation: Test loading the index\n",
    "print(\"\\nValidation: Testing index loading...\")\n",
    "test_index = InvertedIndex.read_index(OUTPUT_DIR, 'index_body', BUCKET_NAME)\n",
    "print(f\"✓ Index loaded successfully\")\n",
    "print(f\"  Terms in index: {len(test_index.df):,}\")\n",
    "print(f\"  Sample term: {list(test_index.df.keys())[0]}\")\n",
    "print(f\"  Sample df: {list(test_index.df.values())[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
